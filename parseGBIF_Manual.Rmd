---
title: "parseGBIF Manual"
author:
  - Pablo Hendrigo Alves de Melo^[Instituto Federal de Educação, Ciência e Tecnologia de Minas Gerais, pablopains@yahoo.com.br]
  - Nadia Bystriakova^[Natural History Museum, London, n_bystriakova@yahoo.com]
  - Alexandre Monro^[Royal Botanic Gardens, Kew, a.monro@kew.org]
date: "`r Sys.Date()`"
  
output: word_document
template: template.tex
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

# parseGBIF Manual

The parseGBIF package is designed to repackage [Global Biodiversity
Information Facility - GBIF](https://www.gbif.org/) species occurrence
records into a format that optimises its use in further analyses.
Currently occurrence records in GBIF can include several duplicate
digital records, and in the case of vascular plants, for several
physical duplicates of unique collection events (biological
collections). parseGBIF aims to parse these records to a single,
synthetic, record corresponding to a unique collection event to which a
standardized scientific name is associated. It does so by providing
tools to verify and standardize species scientific names, score the
quality of both the naming of a record and of its associated spatial
data, and to use those scores to synthesise and parse duplicate records
into unique collection events. This Manual provides a brief introduction
to parseGBIF, with more information available from Help pages accessed
via the help fuction. We believe that this package will be of particular
use for analyses of plant occurrence data.

## Installation

You can install the development version of parseGBIF from
[GitHub](https://github.com/pablopains/parseGBIF). To install parseGBIF,
run

```{r example_install, eval=FALSE}
devtools::install_github("pablopains/parseGBIF")
```

Please site parseGBIF as:

```{r example_citation, eval=TRUE}
print(citation("parseGBIF"), bibtex = FALSE)
```

## Example

**Getting species occurrence records from GBIF**

### 1. GBIF data preparation

#### 1.1. Getting occurrence data of the species records from GBIF

1.1.1. Access a registered account in [GBIF](gbif.org)

1.1.2. Filter occurrences using available fields, for instance:

-   Basis of record: *Preserved specimen*
-   Occurrence status: *present*
-   Scientific name: *Botanical family name* (e.g. Achatocarpaceae) or
    **filter by other fields**

1.1.3. Request to download information in **DARWIN CORE ARCHIVE FORMAT**

1.1.4. Download compressed file and unzip downloaded file

1.1.5. Use the **occurrence.txt** file as input to the
prepare_gbif_occurrence_data(gbif_occurrece_file = 'occurrence.txt')
function

#### 1.2. Preparing occurrence data downloaded from GBIF

To prepare occurrence data downloaded from GBIF to be used by parseGBIF
functions, run prepare_gbif_occurrence_data.

```{r example_prepare_gbif_occurrence_data, eval=TRUE, message=FALSE, warning=FALSE}
library(parseGBIF)
  
occ_file <- 'https://raw.githubusercontent.com/pablopains/parseGBIF/main/dataGBIF/Achatocarpaceae/occurrence.txt'
  
occ <- parseGBIF::prepare_gbif_occurrence_data(gbif_occurrece_file = occ_file, columns = 'standard')
  
head(occ)
```

When parsing data, the user can choose to select "standard" or "all"
fields (columns). The "standard" format has 55 data fields (columns),
and the "all" format, 257 data fields (columns).

```{r example_select_gbif_fields, eval=TRUE, message=FALSE, warning=FALSE}
col_standard <- parseGBIF::select_gbif_fields(columns = 'standard')
  
str(col_standard)

col_all <- parseGBIF::select_gbif_fields(columns = 'all')

str(col_all)
```

#### 1.3. Extracting GBIF issues

GBIF recognises and documents several issues relating to the data fields
for an individual record. The issue field stores terms that represent an
enumeration of GBIF validation rules. Issues can lead to errors or
unexpected data. The issues fields are therefore a valuable source of
information when assessing the quality of a record. In order to help
GBIF and the data publishers improve the data, GBIF flag records with
various issues that they have encountered. These issues can be used as
filters applied to occurrence searches. Not all issues indicate bad
data, some flagthe fact that GBIF has altered values during processing.
The values of EnumOccurrenceIssue will be used by the function
extract_gbif_issue as a model to tabulate the GBIF issues of each
record, individualizing them, in columns.TRUE or FALSE, flagging whether
the issue applies or not for each record.

```{r aaa, eval=TRUE, message=FALSE, warning=FALSE}
data(EnumOccurrenceIssue)

colnames(EnumOccurrenceIssue)
  
head(dplyr::arrange(EnumOccurrenceIssue,desc(score)))
```

```{r example_extract_gbif_issue, eval=TRUE, message=FALSE, warning=FALSE}
occ_gbif_issue <- parseGBIF::extract_gbif_issue(occ = occ)

names(occ_gbif_issue)

head(occ_gbif_issue$summary)
```

### 2. Check species names against WCVP database

The World Checklist of Vascular Plants (WCVP) database is available from
the (Royal Botanic Gardens,
Kew)[<https://powo.science.kew.org/about-wcvp>]. It can be downloaded to
a folder of the user's choice or into memory using get_wcvp function.
The output has 33 columns.

```{r example_wcvp_names, eval=TRUE, message=FALSE, warning=FALSE}
data(wcvp_names_Achatocarpaceae)
wcvp_names <- wcvp_names_Achatocarpaceae
  
# wcvp_names <- wcvp_get_data(read_only_to_memory = TRUE)$wcvp_names
  
colnames(wcvp_names)
```

Species' names can be checked against WCVP database one by one, or in a
batch mode. To verify individual names, the function wcvp_check_name is
used.

```{r example_wcvp_check_name, eval=TRUE, message=FALSE, warning=FALSE}
name.checked <- wcvp_check_name(searchedName = 'Achatocarpus mollis H.Walter',
               wcvp_names = wcvp_names,
               if_author_fails_try_without_combinations = TRUE)
name.checked[,c(3:5,22,23,40)]
```

To check names in a batch mode, there is wcvp_check_name_batch function.
It uses the occurrence data (occ) and WCVP names list (wcvp_names)
generated in the previous steps.

```{r example_wcvp_check_name_batch, eval=TRUE, message=FALSE, warning=FALSE}
names.checked <- wcvp_check_name_batch(occ = occ,
                                                 wcvp_names =  wcvp_names,
                                                 if_author_fails_try_without_combinations = TRUE,
                                                 wcvp_selected_fields = 'standard')
  
names(names.checked)
  
head(names.checked$summary)
```

To bring species' names into line with the format used by WCVP, the
function standardize_scientificName inserts a space between the hybrid
separator (x) and specific epithet, and also standardizes abbreviations
of infrataxa (variety, subspecies, form).

```{r example_standardize_scientificName, eval=TRUE, message=FALSE, warning=FALSE}
# hybrid separator
standardize_scientificName('Leucanthemum ×superbum (Bergmans ex J.W.Ingram) D.H.Kent')

# variety 

standardize_scientificName('Urera baccifera var. angustifolia Wedd.')

# subspecies
standardize_scientificName('Platymiscium pubescens subsp. fragrans (Rusby) Klitg.')
```

The function collectors_get_name returns the last name of the main
collector in recordedBy field. It standardizes the text string to
replace non-ascii characters.

```{r example_collectors_get_name, eval=TRUE, message=FALSE, warning=FALSE}
# library(parseGBIF)

collectors_get_name('Müller, W.')

collectors_get_name("PEDRO ACEVEDO-RODRÍGUEZ|A. SIACA|GEORGE R. PROCTOR|JULIE F. BARCELONA|J.A. CEDEÑO|P. LEWIS|R. O'REILLY|E. SANTIAGO")

collectors_get_name("BORNMÜLLER, JOSEPH FRIEDRICH NICOLAUS")

collectors_get_name("Botão, S.R.; Machado, F.P.")

collectors_get_name('Melo, P.H.A, Bystriakova, N. & Monro, A.')

collectors_get_name('Monro, A.; Bystriakova, N. & Melo, P.H.A')

collectors_get_name('Bystriakova, N., Monro, A.,Melo, P.H.A')

```

### 3. Collectors Dictionary

To extract the last name of the main collector based on the recordedBy
field and assemble a list relating the last name of the main collector
and the raw data from the recordedBy, use the
collectors_prepare_dictionary function. It uses the occurrence data
(occ) generated in the previous step.

#### 3.1 Prepare dictionary collectors

```{r example_collectors_prepare_dictionary, eval=TRUE, message=FALSE, warning=FALSE}

collectorsDictionary.dataset <- collectors_prepare_dictionary(occ = occ)

head(collectorsDictionary.dataset)
```

#### 3.2 Check the main collector's last name

It is recommended to check the main collector's last name in the
nameRecordedBy_Standard field. Our goal is to standardize the main
collector's last name, which is automatically extracted from the
recordedBy field. We do so by standardizing the text string so that all
characters are replaced by uppercase and non-ascii characters, so that
collector reponsible for a collection event is always recorded using the
same string of characters.

If the searched recordedBy entry is present in the collector's
dictionary, the function retrieves the last name of the main collector
with reference to the recordedBy field (in which case the
CollectorDictionary field will be flagged as 'checked'), otherwise, the
function will return the last name of the main collector, extracted
automatically from the recordedBy field .

Once verified, the collector's dictionary can be reused in the future.

```{r example_check_collectorsDictionary, eval=TRUE, message=FALSE, warning=FALSE}

  file.collectorsDictionary.dataset <-  'file_collectorsDictionary_dataset.csv'

  write.csv(collectorsDictionary.dataset,
            file.collectorsDictionary.dataset, 
            row.names = FALSE, 
            fileEncoding = "UTF-8", 
            na = "")
```

#### 3.3 Generating the collection event key

This generates a key to identify the physical and digital duplicates, of
a given collection event. It combines the primary collector's surname,
the collector's number and the botanical family, a key is created
(family + recordByStandardized + recordNumber_Standard) that allows
grouping the duplicates of the same unique collection event.

It also identifiesnew collectors to be added to the collector dictionary
and that can be reused in the future. in the future.

```{r example_generate_collection_event_key, eval=TRUE, message=FALSE, warning=FALSE}

  occ_collectorsDictionary <- generate_collection_event_key(occ=occ,
                                        collectorDictionary_checked_file = file.collectorsDictionary.dataset)

  names(occ_collectorsDictionary)
  
  head(occ_collectorsDictionary$occ_collectorsDictionary[,c(1,3)])

```

### 4. Selecting the master digital voucher

To group duplicates and choose the digital voucher:

Unique collection events can result in many 'duplicate' GBIF records. We
designate one of these 'duplicate' records as the master digital
voucher, to which data from other duplicate vouchers can be merged (see
export_data):

-   **Where the collection event key for grouping duplicates is
    complete**, then duplicates can be grouped / parsed. To do so, we
    evaluate record completeness. Record completeness is calculated
    based on data-quality scores for the information in the following
    fields: recordedBy, recordNumber, year, institutionCode,
    catalogNumber, locality, municipality, countryCode, stateProvince
    and fieldNotes. The spatial coordinates associated with each
    duplicate are ranked using a score for the quality of the geospatial
    information. This score is calculated using the issues listed in the
    GBIF table, EnumOccurrenceIssue.\
    A score is calculated based on these issues (see above). The
    duplicate with the highest total score is assigned as the master
    voucher for the unique collection event. Missing information
    contained in duplicate records of the unique collection event can
    then be merged into the master digital voucher (see export_data).

-   **Where the collection event key is incomplete**, unique collection
    event duplicates cannot be parsed. In this case, each record is
    considered as a unique collection event, without duplicates.
    However, to know the integrity of the information, record
    completeness and quality of the geospatial information, are
    evaluated as described above.

**How is the quality score calculated?** parseGBIF_digital_voucher = The
duplicate with the highest total score, sum of record completeness +
quality of geospatial information.

**How is record completeness calculated?** The quality of the duplicate
records associated with each collection event key is measured as the
completeness of a record, using the sum of a number of flags (see below)
equal to TRUE.

**Flags used to calculate record completeness**

-   Is there information about the collector?
-   Is there information about the collection number?
-   Is there information about the year of collection?
-   Is there information about the institution code?
-   Is there information about the catalog number?
-   Is there information about the locality?
-   Is there information about the municipality of collection?
-   Is there information about the state/province of collection?
-   Is there information about the field notes?

**The quality of geospatial information is based on geographic issues
raised by GBIF.** GIBF issues relating to geospatial data were
classified into three classes based on the data quality scores that we
assigned to each of the following GBIF issues recorded in the
EnumOccurrenceIssue.

-   Issue does not affect coordinating accuracy, with selection_score
    equal to -1
-   Issue has potential to affect coordinate accuracy, with
    selection_score equal to -3
-   Records with a selection_score equal to -9 are excluded.

```{r example_select_digital_voucher, eval=TRUE, message=FALSE, warning=FALSE}

occ_digital_voucher <- parseGBIF::select_digital_voucher(occ = occ,
                                                         occ_gbif_issue = occ_gbif_issue$occ_gbif_issue,
                                                         occ_wcvp_check_name = names.checked$occ_wcvp_check_name ,
                                                         occ_collectorsDictionary = occ_collectorsDictionary$occ_collectorsDictionary)

  names(occ_digital_voucher)
  
  colnames(occ_digital_voucher$occ_digital_voucher)
    
  file.occ_digital_voucher <-  'occ_digital_voucher.csv'

  write.csv(occ_digital_voucher$occ_digital_voucher,
            file.occ_digital_voucher, 
            row.names = FALSE, 
            fileEncoding = "UTF-8", 
            na = "")
```

### 5. Export of results

For each unique collection event key, complete or incomplete, outputs
will be created which combine information from duplicate records and
generate a single unique collection event record to replace them.

The main output fields relating to taxonomic identification and geographic coordinates:

-   parseGBIF_sample_taxon_name = scientific name chosen as taxonomic identification for unique collection event
-   parseGBIF_number_taxon_names = number of scientific names found in duplicates of unique collection event parseGBIF_sample_taxon_name_status
= status of choice of 'identified', 'divergent identifications', 'unidentified'
-   parseGBIF_unidentified_sample = if unique collection event has taxonomic identification
-   parseGBIF_decimalLatitude = latitude in decimal degrees 
-   parseGBIF_decimalLongitude = longitude in decimal degrees 
-   parseGBIF_useful_for_spatial_analysis = whether the coordinates are useful for spatial analysis.

**How is the taxon binomial attributed to the unique collection event selected?**

-   **Where the unique collection event key is complete**: The accepted
    TAXON_NAME selected is that which is most frequently applied to the
    duplicate vouchers at or below the rank of species. Where two named
    are applied with equal frequency then a mechanical approach, using
    alphabetical order, is applied, the first listed TAXON_NAME being
    chosen. Where there is no identification, at or below the rank of
    species, then the unique collection event, the unique collection
    event is indicated as unidentified.

-   **Where the unique collection event key is incomplete**: Where the
    unique collection event key is incomplete, then each record is
    treated as a unique collection event. If there is no identification,
    at or below the rank of species, then the unique collection event is
    classified as unidentified. 

**Geospatial information**

If the master voucher does not have geographic coordinates, we will seek coordinates from the duplicate records associated with it. Finally, the records are separated into three sets of data:

-  **useable_data** Where unique collection event with taxonomic identification and geographic coordinates are complete. This represents the useable dataset.  

-  **unusable_data** Where unique collection event without taxonomic identification and/or geographic coordinates.  

-  **duplicates** The duplicates of unique collection events complete / incomplete.  

With this, it is possible to perform:

Merge information between fields of duplicates of a unique collection event to create a synthetic record for each unique collection event, Compare the frequency of content in fields Generate a work package summary.  

For each complete unique collection event key, data fields that are empty in the digital voucher record will be populated with data from the respective duplicates. During content merging, we indicate fields associated with the description, location, and data of the unique collection event. By default, fields_to_merge parameter of export_data function contains: 
-  Ctrl_fieldNotes
-  Ctrl_year
-  Ctrl_stateProvince
-  Ctrl_municipality
-  Ctrl_locality
-  Ctrl_countryCode
-  Ctrl_eventDate
-  Ctrl_habitat
-  Ctrl_level0Name
-  Ctrl_level1Name
-  Ctrl_level2Name
-  Ctrl_level3Name

**export_data function return a list with 10 data frames**:

-  **all_data** All records processed, merged Unique collection events complete / incomplete and their duplicates 
-  **useable_data_merge** Merged useable dataset
-  **useable_data_raw** Raw useable dataset
-  **duplicates** Duplicates of unique collection events of useable and useable datasets
-  **unusable_data_merge** Merged unusable dataset. It is NA if merge_unusable_data is FALSE.
-  **unusable_data_raw** Raw unusable dataset
-  **parseGBIF_general_summary**
-  **parseGBIF_merge_fields_summary**
-  **parseGBIF_merge_fields_summary_useable_data**
-  **parseGBIF_merge_fields_summary_unusable_data** It is NA if merge_unusable_data is FALSE

```{r example_export, eval=TRUE, message=FALSE, warning=FALSE}

results <- export_data(occ_digital_voucher_file = file.occ_digital_voucher,
                           merge_unusable_data = TRUE)

names(results)

results$parseGBIF_general_summary
results$parseGBIF_merge_fields_summary
results$parseGBIF_merge_fields_summary_useable_data

NROW(results$all_data)
NROW(results$useable_data_merge)
NROW(results$useable_data_raw)
NROW(results$duplicates)

file.all <- 'parseGBIF_all_data.csv'
write.csv(results$all_data,
          file.all, 
          row.names = FALSE, 
          fileEncoding = "UTF-8", 
          na = "")

file.summary <- 'parseGBIF_general_summary.csv'
write.csv(results$parseGBIF_general_summary,
          file.summary, 
          row.names = FALSE, 
          fileEncoding = "UTF-8", 
          na = "")
  
file.summary <- 'parseGBIF_merge_fields_summary.csv'
write.csv(results$parseGBIF_merge_fields_summary,
          file.summary, 
          row.names = FALSE, 
          fileEncoding = "UTF-8", 
          na = "")

file.summary <- 'parseGBIF_merge_fields_summary_complete.csv'
write.csv(results$parseGBIF_merge_fields_summary_complete,
          file.summary, 
          row.names = FALSE, 
          fileEncoding = "UTF-8", 
          na = "")
```

### Accessing map of merged information and frequency of content in fields

-  Merged information between fields of duplicates of a unique collection event

```{r example_json1, eval=TRUE, message=FALSE, warning=FALSE}
index <- results$useable_data_merge$Ctrl_key_family_recordedBy_recordNumber %in% 'ACHATOCARPACEAE_ZARDINI_5592'

print('merged fields')
print(jsonlite::fromJSON(results$useable_data_merge$parseGBIF_merged_fields[index==TRUE]))

print('merged fields map')
print(jsonlite::fromJSON(results$useable_data_merge$parseGBIF_duplicates_map[index==TRUE]))

```

-  Frequency of content in fields between fields of duplicates of a unique collection event

```{r example_json2, eval=TRUE, message=FALSE, warning=FALSE}

print('Frequency of content in fields')
print(jsonlite::fromJSON(results$useable_data_merge$parseGBIF_freq_duplicate_or_missing_data[index==TRUE]))
```


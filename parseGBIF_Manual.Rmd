---
title: "parseGBIF Manual"

author:
  - Pablo Hendrigo Alves de Melo^[Instituto Federal de Educação, Ciência e Tecnologia de Minas Gerais, pablopains@yahoo.com.br]
  - Nadia Bystriakova^[Natural History Museum, London, n_bystriakova@yahoo.com]
  - Alexandre Monro^[Royal Botanic Gardens, Kew, a.monro@kew.org]
  
date: "`r Sys.Date()`"
  
output: word_document

template: template.tex
---

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

# parseGBIF Manual

The parseGBIF package is designed to repackage [Global Biodiversity Information Facility - GBIF](https://www.gbif.org/) species occurrence records  into a format that optimises its use in further analyses. Currently occurrence records in GBIF can include several duplicate digital records, and in the case of vascular plants, for several physical duplicates of unique collection events (biological collections).
parseGBIF aims to parse these records to a single, synthetic, record corresponding to a unique collection event to which a standardized scientific name is associated. It does so by providing tools to verify and standardize species scientific names, score the quality of both the naming of a record and of its associated spatial data, and to use those scores to synthesise and parse duplicate records into unique collection events. This Manual provides a brief introduction to parseGBIF, with more information available from Help pages accessed via the help fuction. We believe that this package will be of particular use for analyses of plant occurrence data.

## Installation

You can install the development version of parseGBIF from [GitHub](https://github.com/pablopains/parseGBIF).
To install parseGBIF, run 

```{r example_install, eval=FALSE}
devtools::install_github("pablopains/parseGBIF")
```

Please site parseGBIF as:
```{r example_citation, eval=TRUE}
print(citation("parseGBIF"), bibtex = FALSE)
```

## Example
__Getting species occurrence records from GBIF__

### 1. GBIF data preparation
#### 1.1. Getting occurrence data of the species records from GBIF
  1.1.1. Access a registered account in [GBIF](gbif.org) 
  
  1.1.2. Filter occurrences using available fields, for instance:
  
  * Basis of record: _Preserved specimen_
  * Occurrence status: _present_
  * Scientific name: _Botanical family name_ (e.g. Achatocarpaceae) or __filter by other fields__

  1.1.3. Request to download information in __DARWIN CORE ARCHIVE FORMAT__
  
  1.1.4. Download compressed file and unzip downloaded file
  
  1.1.5. Use the __occurrence.txt__ file as input to the prepare_gbif_occurrence_data(gbif_occurrece_file = 'occurrence.txt') function
  
#### 1.2. Preparing occurrence data downloaded from GBIF 

To prepare occurrence data downloaded from GBIF to be used by parseGBIF functions, run prepare_gbif_occurrence_data.

```{r example_prepare_gbif_occurrence_data, eval=TRUE, message=FALSE, warning=FALSE}
  library(parseGBIF)
  
  occ_file <- 'https://raw.githubusercontent.com/pablopains/parseGBIF/main/dataGBIF/Achatocarpaceae/occurrence.txt'
  
  occ <- parseGBIF::prepare_gbif_occurrence_data(gbif_occurrece_file = occ_file, columns = 'standard')
  
  head(occ)
```

When parsing data, the user can choose between “standard” and “all” columns to be selected. The “standard” format has 54 data fields (columns), and the “all” format, 257 data fields (columns).

```{r example_select_gbif_fields, eval=TRUE, message=FALSE, warning=FALSE}
  col_standard <- parseGBIF::select_gbif_fields(columns = 'standard')
  
  str(col_standard)

  col_all <- parseGBIF::select_gbif_fields(columns = 'all')

  str(col_all)
```


#### 1.3. Extracting GBIF issue 

There are many things that can go wrong and we continously encounter unexpected data.
In order to help us and publishers improve the data, we flag records with various issues
that we have encountered. This is also very useful for data consumers as you can include
these issues as filters in occurrence searches. Not all issues indicate bad data.
Some are merley flagging the fact that GBIF has altered values during processing.
On the details page of any occurrence record you will see the list of issues in the notice at the bottom.

```{r aaa, eval=TRUE, message=FALSE, warning=FALSE}
  data(EnumOccurrenceIssue)

  colnames(EnumOccurrenceIssue)
  
  help(EnumOccurrenceIssue)
  print(?parseGBIF::EnumOccurrenceIssue)
```


```{r example_extract_gbif_issue, eval=TRUE, message=FALSE, warning=FALSE}

 occ_gbif_issue <- parseGBIF::extract_gbif_issue(occ = occ)

 names(occ_gbif_issue)

 head(occ_gbif_issue$summary)
```

### 2. Check species names against WCVP database

The World Checklist of Vascular Plants (WCVP) database is available from the (Royal Botanic Gardens, Kew) [https://powo.science.kew.org/about-wcvp]. It can be downloaded to a folder of the user’s choice or into memory using get_wcvp function. The output has 33 columns.

```{r example_wcvp_names, eval=TRUE, message=FALSE, warning=FALSE}
  data(wcvp_names_Achatocarpaceae)
  wcvp_names <- wcvp_names_Achatocarpaceae
  
  # wcvp_names <- wcvp_get_data(read_only_to_memory = TRUE)$wcvp_names
  
  colnames(wcvp_names)
```

Species’ names can be checked against WCVP database one by one or in a batch mode. To verify individual names, the function wcvp_check_name is used.

```{r example_wcvp_check_name, eval=TRUE, message=FALSE, warning=FALSE}
  name.checked <- wcvp_check_name(searchedName = 'Achatocarpus mollis H.Walter',
                 wcvp_names = wcvp_names,
                 if_author_fails_try_without_combinations = TRUE)
  name.checked[,c(3:5,22,23,40)]
```

To check names in a batch mode, there is wcvp_check_name_batch function. It uses the occurrence data (occ) and WCVP names list (wcvp_names) generated in the previous steps.

```{r example_wcvp_check_name_batch, eval=TRUE, message=FALSE, warning=FALSE}
  names.checked <- wcvp_check_name_batch(occ = occ,
                                                   wcvp_names =  wcvp_names,
                                                   if_author_fails_try_without_combinations = TRUE,
                                                   wcvp_selected_fields = 'standard')
  
  names(names.checked)
  
  head(names.checked$summary)
```

To bring species’ names into line with the format used by WCVP, the function standardize_scientificName inserts space between the hybrid separator (x) and specific epithet, and also standardizes abbreviations of infrataxa (variety, subspecies, form).

```{r example_standardize_scientificName, eval=TRUE, message=FALSE, warning=FALSE}
# hybrid separator
standardize_scientificName('Leucanthemum ×superbum (Bergmans ex J.W.Ingram) D.H.Kent')

# variety 
standardize_scientificName('Platymiscium pubescens subsp. fragrans (Rusby) Klitg.')

# subspecies

```

The funtion get_lastNameRecordedB returns the last name of the main collector in recordedBy field.

```{r example_collectors_get_name, eval=TRUE, message=FALSE, warning=FALSE}
# library(parseGBIF)
collectors_get_name('Melo, P.H.A, Bystriakova, N. & Monro, A.')

collectors_get_name('Monro, A.; Bystriakova, N. & Melo, P.H.A')

collectors_get_name('Bystriakova, N., Monro, A.,Melo, P.H.A')

```


### 3. Collectors Dictionary

To extract the last name of the main collector based on the recordedBy field and assemble a list relating the last name of the main collector and the raw data from the recordedBy, use the collectors_prepare_dictionary function. It uses the occurrence data (occ) generated in the previous step.

#### 3.1 Prepare dictionary collectors

```{r example_collectors_prepare_dictionary, eval=TRUE, message=FALSE, warning=FALSE}

collectorsDictionary.dataset <- collectors_prepare_dictionary(occ = occ)

head(collectorsDictionary.dataset)
```

#### 3.2 Check the main collector's last name
It is recommended to check the main collector's last name in the nameRecordedBy_Standard field. The goal is to standardize the main collector's last name, which was automatically extracted from the recordedBy field, standardized in uppercase and with non-ascii characters replaced, so that a botanical collector is always recognized by the same last name.

If the searched recordedBy exists in the collector's dictionary, the function retrieves the last name of the main collector referring to the recordedBy (in this case the CollectorDictionary field will be indicated with 'checked'), otherwise, it returns the last name of the main collector, extracted automatically from the recordedBy field .

Once verified, the collector's dictionary can be reused in the future.

```{r example_check_collectorsDictionary, eval=TRUE, message=FALSE, warning=FALSE}

  file.collectorsDictionary.dataset <-  'file_collectorsDictionary_dataset.csv'

  write.csv(collectorsDictionary.dataset,
            file.collectorsDictionary.dataset, 
            row.names = FALSE, 
            fileEncoding = "UTF-8", 
            na = "")
```


#### 3.3 Update dictionary collectors

Create a key to group duplicates of a sample.

It also returns new collectors to be added to the collector dictionary that can be reused in the future.


```{r example_generate_collection_event_key, eval=TRUE, message=FALSE, warning=FALSE}

  occ_collectorsDictionary <- generate_collection_event_key(occ=occ,
                                        collectorDictionary_checked_file = file.collectorsDictionary.dataset)

  names(occ_collectorsDictionary)
  
  head(occ_collectorsDictionary$occ_collectorsDictionary[,c(1,3)])

```


 
### 4. Select digital voucher

To group duplicates and choose the digital voucher:

1) If the key for grouping duplicates is complete with collector information and collection number, sample duplicates can be grouped. In this case, the voucher with the highest score is selected among the duplicates in the sample.

2) If the key to group duplicates is incomplete, sample duplicates cannot be grouped due to missing collector information and/or collection number. In this case, each record is considered a sample, without duplicates, and a voucher is selected for each sample.

#### How is the information score calculated? 

moreInformativeRecord = sum of **textual quality** + **quality of geospatial information**.  

#### How is the **quality of textual** information calculated?

##### The **Text quality** is the sum of the number of flags with text quality equal to TRUE.  

Is there information about the collector?  
Is there information about the collection number?  
Is there information about the year of collection?  
Is there information about the institution code?  
Is there information about the catalog number?  
Is there information about the locality?  
Is there information about the municipality of collection?  
Is there information about the state/province of collection?  
Is there information about the bibliographic citation?  

#### How is the **quality of geospatial information** calculated?

##### The **quality of geospatial information** is based on geographic issues made available by GBIF.

GIBF issues on the quality of geospatial information were classified into three levels.

* Not applicable, with selection_score equal to 0
* Does not affect coordinating accuracy, with selection_score equal to -1
* Potentially affect coordinate accuracy, with selection_score equal to -3
* Records to be excluded from spatial analysis, with selection_score equal to -9











```{r example_select_digital_voucher, eval=TRUE, message=FALSE, warning=FALSE}

occ_digital_voucher <- parseGBIF::select_digital_voucher(occ = occ,
                                                         occ_gbif_issue = occ_gbif_issue$occ_gbif_issue,
                                                         occ_wcvp_check_name = names.checked$occ_wcvp_check_name ,
                                                         occ_collectorsDictionary = occ_collectorsDictionary$occ_collectorsDictionary)

  names(occ_digital_voucher)
  
  colnames(occ_digital_voucher$occ_digital_voucher)
    
  file.occ_digital_voucher <-  'occ_digital_voucher.csv'

  write.csv(occ_digital_voucher$occ_digital_voucher,
            file.occ_digital_voucher, 
            row.names = FALSE, 
            fileEncoding = "UTF-8", 
            na = "")
```

### 5. Export of results

From the selected digital voucher.

* Select the taxonomic identification of the sample,
* Select geographic coordinates,
* Merge information between fields of duplicates of a sample,
* Compare the frequency of content in fields
* Generate data summary
* Export results

#### How is the taxonomic identification of the sample chosen?

1) When the key to group the duplicates is complete:

The accepted TAXON_NAME identified at or below the rank of species and the most frequent among the duplicates is chosen.

In case of a tie in frequency, a mechanical approach, using alphabetical order, is applied, the first accepted TAXON_NAME identified up to or below the specific level is chosen.

If there is no identification, at or below the rank of species, for the sample, the sample is indicated as unidentified.

2) When the key to group the duplicates is incomplete:

In the case where the key to group the duplicates is incomplete, the accepted TAXON_NAME identified at or below the rank of species is used. If there is no identification, at or below the rank of species, the sample is indicated as unidentified.

#### return list with 10 data frames

* __all_data__ All records processed, merged Unique collection events complete / incomplete and their duplicates 
* __unique_collection_event_complete_merge__ Merged Unique collection events complete
* __unique_collection_event_complete_raw__ Raw Unique collection events complete
* __duplicates__ Duplicates of unique collection events complete / incomplete
* __unique_collection_event_incomplete_merge__ Merged Unique collection events incomplete, It is NA if merge_unusable_data is FALSE.
* __unique_collection_event_incomplete_raw__ Raw Unique collection events incomplete
* __parseGBIF_general_summary__
* __parseGBIF_merge_fields_summary__
* __parseGBIF_merge_fields_summary_complete__
* __parseGBIF_merge_fields_summary_incomplete__ It is NA if merge_unusable_data is FALSE


```{r example_export, eval=TRUE, message=FALSE, warning=FALSE}

results <- export_data(occ_digital_voucher_file = file.occ_digital_voucher,
                           merge_unusable_data = TRUE)

names(results)

results$parseGBIF_general_summary
results$parseGBIF_merge_fields_summary
results$parseGBIF_merge_fields_summary_complete

NROW(results$all_data)
NROW(results$unique_collection_event_complete_merge)
NROW(results$unique_collection_event_incomplete_raw)
NROW(results$duplicates)

  file.all <- 'parseGBIF_all_data.csv'
  write.csv(results$all_data,
            file.all, 
            row.names = FALSE, 
            fileEncoding = "UTF-8", 
            na = "")

  file.summary <- 'parseGBIF_general_summary.csv'
  write.csv(results$parseGBIF_general_summary,
            file.summary, 
            row.names = FALSE, 
            fileEncoding = "UTF-8", 
            na = "")
  
  file.summary <- 'parseGBIF_merge_fields_summary.csv'
  write.csv(results$parseGBIF_merge_fields_summary,
            file.summary, 
            row.names = FALSE, 
            fileEncoding = "UTF-8", 
            na = "")

  file.summary <- 'parseGBIF_merge_fields_summary_complete.csv'
  write.csv(results$parseGBIF_merge_fields_summary_complete,
            file.summary, 
            row.names = FALSE, 
            fileEncoding = "UTF-8", 
            na = "")


```
